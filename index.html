
<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="text-to-video generation evaluation">
  <meta name="keywords" content="multimodal evaluation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MantisScore</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <link rel="icon" href="https://cdn-icons-png.flaticon.com/512/954/954593.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js"></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MantisScore: A Reliable Fine-grained Metric for Video Generation</h1>
              <div class="is-size-5 publication-authors">
                  <span class="author-block">
                    <sup>1,2</sup><a href="" style="font-weight:normal;">Xuan He*</a>,
                  </span>
                  <span class="author-block">
                    <sup>1</sup><a href="https://jdf-prog.github.io/" style="font-weight:normal;">Dongfu Jiang*</a>,
                  </span>
                  <span class="author-block">
                    <sup>1</sup><a href="" style="font-weight:normal;">Ge Zhang</a>,
                  </span>
                  <span class="author-block">
                    <sup>1</sup><a href="https://kuwingfung.github.io/" style="font-weight:normal;">Max Ku</a>,
                  </span>
                  <span class="author-block">
                    <sup>1</sup><a style="font-weight:normal;">Achint Soni</a>,
                  </span>
                  <span class="author-block">
                    <sup>1</sup><a style="font-weight:normal;">Sherman Siu</a>,
                  </span>
                  <span class="author-block">
                    <sup>1</sup><a style="font-weight:normal;">Haonan Chen</a>,
                  </span>
                  <span class="author-block"> 
                    <sup>1</sup><a style="font-weight:normal;">Abhranil Chandra</a>,
                  </span>
                  <span class="author-block">
                    <span class="author-block">
                    <sup>1</sup><a style="font-weight:normal;">Ziyan Jiang</a>,
                  </span>
                  <span class="author-block">
                    <sup>1</sup><a style="font-weight:normal;">Aaran Arulraj</a>,
                  </span>
                  <span class="author-block">
                    <sup>3</sup><a style="font-weight:normal;">Kai Wang</a>,
                  </span>
                  <span class="author-block">
                    <sup>1</sup><a style="font-weight:normal;">Quy Duc Do</a>,
                  </span>
                  <span class="author-block">
                    <sup>1</sup><a style="font-weight:normal;">Yuansheng Ni</a>,
                  </span>
                  <span class="author-block">
                    <sup>2</sup><a style="font-weight:normal;">Bohan Lyu</a>,
                  </span>
                  <span class="author-block">
                    <sup>1</sup><a style="font-weight:normal;">Yaswanth Narsupalli</a>,
                  </span>
                  <span class="author-block">
                    <sup>1</sup><a style="font-weight:normal;">Rongqi Fan</a>,
                  </span>
                  <span class="author-block">
                    <sup>1</sup><a style="font-weight:normal;">Zhiheng Lyu</a>,
                  </span>
                  <span class="author-block">
                    <sup>4</sup><a href="https://yuchenlin.xyz/" style="font-weight:normal;">Bill Yuchen Lin</a>,
                  </span>
                  <span class="author-block">
                    <sup>1</sup><a href="https://wenhuchen.github.io/" style="font-weight:normal;">Wenhu Chen</a>
                  </span>
                
              </div>

            <div class="is-size-5 publication-authors">
              <p></p>
                            <span class="author-block">

                    *Equal Contribution
                            </span>
                          </div>
                          
            <div class="is-size-5 publication-authors">
                            <span class="author-block">
                    <sup>1</sup>University of Waterloo,
                    <sup>2</sup>Tsinghua University,
                    <sup>3</sup>University of Toronto,
                    <sup>4</sup>AI2
                            </span>
                     </div>   

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <!-- add arXiv link -->
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/TIGER-AI-Lab/MantisScore/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://huggingface.co/datasets/TIGER-Lab/Video_Eval" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Video-Eval </span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://huggingface.co/spaces/TIGER-Lab/MantisScore" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      ðŸ¤—
                    </span>
                    <span> Spaces</span>
                  </a>
                </span>

                <!-- <span class="link-block">
                  <a href="https://twitter.com/DongfuJiang/status/1786552974598078677" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-twitter"></i>
                    </span>
                    <span>Twitter</span>
                  </a>
                </span> -->

              </div>
            </div>

            <centering>
              <div style="text-align: center;">
                <img id="teaser" width="85%" src="static/images/teaser.png">     
              </div>
            
            </centering> 

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section"  style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>

              The recent years have witnessed great advances in text-to-video generation. 
              However, the video evaluation metrics have lagged significantly behind, 
              which fails to produce an accurate and holistic measure of the generated videos' quality. 
              The main barrier is the lack of high-quality human rating data. 
              In this paper, we release VideoEval, the first large-scale multi-aspect video evaluation dataset. 
              VideoEval consists of high-quality human-provided ratings for 5
              video evaluation aspects on the 37.6K videos
              generated from 11 existing popular video generative models. 
              We train MantisScore based on VideoEval to enable automatic video quality assessment. 
              Experiments show that the Spearman correlation between MantisScore
              and humans can reach 77.1 on VideoEval 
              test, beating the prior best metrics by about
              50 points. Further result on the held-out Eval Crafter, GenAI-Bench, and VBench, show that
              MantisScore is highly generalizable and
              still beating the prior best metrics by a remarkable margin. We observe that using Mantis as
              the based model consistently beats that using
              Idefics2 and VideoLLaVA, and the regression based model can achieve better results than the
              generative ones. Due to its high reliability, we
              believe MantisScore can serve as a valuable
              tool for accelerate video generation research.
              <ol type="1">

                <li><b>VideoEval Dataset</b>. We release the first large scale multi-dimension video evaluation dataset, 
                  consisting of 37.6K text-to-video pairs with human annotated scores.<span style="font-size: 100%;">
                  </span></li>
                <li><b>MantisScore</b>. We introduce MantisScore, and video quality evaluator trained on 
                  VideoEval dataset with <a href="https://huggingface.co/TIGER-Lab/Mantis-8B-Idefics2">Mantis-8B-Idefics2</a> as base model.<span style="font-size: 100%;">
                  </span></li>
                <li><b>Evaluation</b>. We test our model on VideoEval-test and 3 other Benchmarks: <a href="https://evalcrafter.github.io/"> EvalCrafter</a>, 
                  <a href="https://github.com/hiamitabha/genai-bench">GenAI-Bench </a> and <a href="https://vchitect.github.io/VBench-project/">VBench</a>, 
                  compared with both feature-based metrics and MLLM prompting methods. 
                  </span></li>
                <li><b>Open Source</b>. Our VideoEval datase, evaluation results and all the training/evaluation code will be released as public.
                  </span></li>

              </ol>  
           </p>
  
          </div>
        </div>
      </div>
        
    </div>
  </section>


  <section class="section">
    <!-- VideoEval Dataset. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">
          VideoEval Dataset</h2>
      </div>
    </div>   
  <div class="container is-max-desktop">
  
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          
          <p>
            <ul>
                VideoEval contains a total of <b>37.6K</b> text-to-video pairs from 11 popular video generative models, 
                with some real-world videos as data augmentation. 
                The videos are annotated by raters for five evaluation dimensions: <b>Visual Quality</b>, <b>Temporal Consistency</b>, <b>Dynamic Degree</b>,
                 <b>Text-to-Video Alignment</b> and <b>Factual Consistency</b>, 
                in 1-4 scoring scale. Below we show two annotated video examples and the detailed description of our VideoEval dataset.
                Please check out <a href="https://huggingface.co/TIGER-Lab/Video_Eval">ðŸ¤— <b>Video-Eval</b></a> on hugging face datasets for usage.
              <p></p>
              <centering>
                <div style="text-align: center;">
                    <img id="anno_example" width="85%" src="static/images/anno_example.png">     
                </div>
              </centering> 
              <p></p>
              <centering>
                <div style="text-align: center;">
                    <img id="dataset" width="85%" src="static/images/dataset.png">     
                </div>
              </centering>
            </ul>
            
  
  <section class="section">
    <!-- MantisScore. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">
          <img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/954/954593.png">
          MantisScore </h2>
      </div>
    </div>   
    <div class="container is-max-desktop">
  
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified"> 
          <p>
            MantisScore is finetuned on VideoEval dataset's 37K training set taking 
            <a href="https://huggingface.co/openai/clip-vit-large-patch14-336">Mantis-8B-Idefics2</a> as base model. 
            We try generation scoring method and regression scoring method, the former one means model's answer is in a template 
            predefined for video quality evaluation while the latter one outputs 5 logits as evaluation scores in 5 dimensions.
         
            Besides, we also make ablation on base model, using Mantis-8B-Idefics2, Idefics2-8B and VideoLLaVA-7B 
            as base models to finetune. Mantis-8B-Idefics2 turns out to have the best performance on video quality evaluation.
          </p>
        </div>          
      </div>
    </div>
  
  
  </section>
  
  


  <section class="section">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3"> Evaluation Results</h2>
      </div>
    </div>
    <div class="container is-max-desktop">
  

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4"><span style="font-size: 100%;">VideoEval-test</span></h2>
        
        <p>
          We test our video evaluator MantisScore on VideoEval-test set, 
          Here is the results of some feature-based metrics like PIQE, CLIP-sim, X-CILIP-Sore etc, 
          MLLM-prompting methods like GPT-4o Gemini-1.5-Pro, etc and our MantisScore. 
          As seen in the table below, MantisScore surpass the best baseline by 54.1 in average on 5 aspects. 
        
        <p></p>
        <centering>
          <div style="text-align: center;">
              <img id="res_video_eval" width="75%" src="static/images/res_video_eval.png">     
          </div>
        </centering>
        
      </div>
    </div>


    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4"><span style="font-size: 100%;">EvalCrafter Benchmark</span></h2>
        
        <p>
          We select 3 dimensions from EvalCrafter that match our evaluation aspects 
          and collect 2500+ videos for test. MantisScore surpass all the baselines in 3 apsects 
          and EvalCrafter(GPT-4V) in Text-to-Video Alignment.
        
        <p></p>
        <centering>
          <div style="text-align: center;">
              <img id="res_eval_crafter" width="40%" src="static/images/res_eval_crafter.png">     
          </div>
        </centering>
        
      </div>
    </div>


    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4"><span style="font-size: 100%;">GenAI-Bench and VBench</span></h2>
        
        <p>
            GenAI-Bench is a multimodal benchmark for MLLM's capability on preference comparison
            for tasks like text-to-video generation, image-editing and others, while 
            VBench is a comprehensive multi-aspect benchmark suite for
            video generative models. For GenAI-Bench we collect 2100+ videos and 
            for VBench select a subset from 5 aspects of VBench, like technical 
            quality, subject consistency, and so on, then subsample 100 unique prompts (2000 videos totally) for testing.
            We use the averaged scores of the five aspects for MLLM prompting baselines and our models to
            give the preference and calculate the pairwise accuracy as performance indicator.
        
        <p></p>
        <centering>
          <div style="text-align: center;">
            <img id="teaser" width="75%" src="static/images/res_genai_vbench.png">     
          </div>
        </centering> 
        
      </div>
    </div>
  
  </section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    
  </div>


</body>

</html>
